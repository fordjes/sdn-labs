---
date: "2016-12-1"
draft: true
weight: 460
title: "Neutron Walk Through"
---
[Click here to find out more about Alta3 Research's SDN Training](https://alta3.com/courses/sdn)

### MONDAY - &#x2B50;REQUIRED&#x2B50;
### Lab Objective
The objective of this lab is explore OpenStack Neutron networking with Open vSwitch.

### Procedure

OpenStack Neutron Packet Walkthrough
#NOTE: Change Red1 to Koopa and Red2 to Goomba


0. From your remote desktop, open a terminal session. Now *ssh* to the OpenStack controller.

    `student@beachhead:/$` `cd` 
    
0. This command shows the expected internal IP of 10.0.0.4:

    `student@controller:~$` `ip a`

0. We can start a continuous ping towards RED2 at 10.0.0.5:

    `student@controller:~$` `ping 10.0.0.5`

0. Now *ssh* into compute1. The first thing that we need is the Neutron port ID for VM RED1:

    `student@controller:~$` `ssh compute1`
    
0. Ask the Nova (compute) service to display the list of interfaces on the VM running on *compute1*.

    `student@compute1:~$` `nova interface-list RED1`

0. We can see the port ID from this command. It is 32 characters, however, we'll only need the first 3 digits. With this info, we can find the Linux bridge in front of RED1:

    `student@compute1:~$` `brctl show`

0. This will show the three Linux bridges present in Koopa. One exists for each VM in this host. Note that the first bridge in the list matches the first ten digits of the Port ID.

0. This first bridge has two ports. The top port leads to the VM's virtual NIC. The QVE port is one end of a veth pair. The other end is on the integration bridge. 

0. Lets do a packet capture on the tap interface to see ICMP traffic.

    `[root@compute1 ~(keystone_demo)]#` `tcpdump -1 (first port number) -n -e icmp`

0. The "icmp" command includes a filter for icmp traffic. The "-e" command filters in layer 2 information to see MAC addresses. The "-n" command disables any attempt of resolution of domain names or OUIs or MAC addresses.

0. You will see ICMP requests and replies between Koopa and Goomba. We'll use this information later.

0. Return to the tap interface on the Linux bridge. The tap interface is where security group rules are implemented. The next part of our lab will explore how security group rules look in IP tables:

    `[root@compute1 ~(keystone_demo)]#` `iptables - S | grep o (first three digits of Koopa's port ID)`

0. "-s" will reveal all rules. Filtering with "grep" will search for only rules associated with Koopa's port ID. The letter "o" before the port ID is just the outbound policy.

0. Find the chain to the right of the inbound tag (physdev-in). This tag means that the chain is leaving or egress from the VM. Now find some of the security rules written into the code.

0. Find the line that contains "Allow DHCP client traffic." This is required to allow the initial spinup of the instance. Another rule prevents DHCP spoofing, which is contained in the line that includes "Prevent DHCP Spoofing by VM"

0. Let's take a closer look at this chain by typing it out:

    `student@compute1:~$` `iptables -S neutron-openvswi-s0e6fb773-9m 

0. This rule only allows traffic from Koopa's allocated IP (which you can find in the code). It also ensures that traffic comes from the assigned MAC address for Koopa, "Allow traffic from defined IP/MAC pairs."

0. Next we will return to the column security chain:

    `student@compute1:~$` `iptables -s neutron-openvswi-sg-chain` 

0. The ICMP traffic is forwarded via a Linux bridge to open vSwitch, specifically the integration bridge. Let's find the associated port in Open vSwitch.

0. Back on Compute Host 1, type the following:

    `student@compute1:~$` `ovs-vsctls show`

0. This shows the integration bridge and its ports.

0. To know what happens after ICMP flow through the intergration bridge on the OVS port, you have the check OpenFlow rules.
    `student@compute1:~$` `watch -n.5 "ovs-ofctl dump-flows br-int \ 

    >
    table=0 | grep -v n_packets=0"` 

0. This command will break this out. The "ovs-ofctl dump-flows br-int" command will show the OpenFlow entries and the flow entries for the integration bridge. To make it easier to observe, we are filtering down to just the first table in the processing pipeline, "table=0." 

0. We will also ignore flow entries with no hits at all, hence the command "grep -v n_packets=0." The watch command "watch -n.5", which is set to half-second intervals, will allow us to easily show which flow entries have their counters incremented.

0. Another command will show us the forwarding table of the integration bridge.

    `student@compute1:~$` `ovs-appctl fdb/show br-int`

0. By typing the command below, we can see the name of the ovs port that port 4 maps to.

    `student@compute1:~$` `ovs-ofctl show br-int`

0. Before moving on, there is another Linux bridge to observe. 

    `student@compute1:~$` `brctl show`

0. This will show you the bridges- let's look at the second bridge.

0. Let's look at iptables one more time to observe ingress rules for the VM Goomba.

    `student@compute1:~$` `iptables -S | grep i399`

0. This command shows the security chain for inbound flows to Goomba.

0. Here you will learn that TCP port 22 traffic is permitted, but only for an IP set with this ID number. The "ipset list" command reveals the numbers of this set:

    `[root@compute1 ~(keystone_demo)]#` `ipset list`

0. In the next scenario, we'll follow the flow when your machines are on the same network, but have different hosts. Koopa must reach Red3. 

0. Let's start by following some ICMP traffic from Koopa to Red3. We'll start with Koopa's CLI and ping across to Red 3.

    `$` `$ ping 10.0.0.6`

0. This shows that the connection works before we get any farther.

    `student@compute1:~$` `ovs-appctl fdb/show br-int` 

0. This shows the forwarding table for the integration bridge.

    `student@compute1:~$` `ovs-ofctl show br-init`

0. This command will reveal which port is br-tun, the next connection in the flow.

    `student@compute1:~$` `ovs-vsctl show`

0. This shows that br-int has a patch port called patch-tun. This is a direct link to br-tun. Lets look at the OpenFlow port number for br-tun:

    `student@compute1:~$` `ovs-ofctl show br-tun`

0. Here we see patch-int maps to OpenFlow port 1. Also, we can see ports 4 and 5 map to interfaces used for VXLan tunnels.

0. So now lets check OpenFlow entries on the tunneling bridge. We can start with the first table, which is table zero:

    `student@compute1:~$` `watch -n.5 "ovs-ofctl dump-flows br-tun table=0"`

0. The first flow entry says that if traffic comes in on OpenFlow port `, resubmit or move it to OpenFlow table 1. So all traffic coming from the integration bridge will go to table 1. 

0. Lets check that table:

    `student@compute1:~$` `watch -n.5 "ovs-ofctl dump-flows br-tun table=1"`

0. Here all the tap rules relate to the distributed routers so none of those will match. A default last line here will match which goes to table 2, so lets look at that table:

    `student@compute1:~$` `watch -n.5 "ovs-ofctl dump-flows br-tun table=2"`
Here on table 2 there are 3 flow entries. The first is an entry for ARP traffic, the second is for unicast traffic. It is identified by matching with a zero on the last bit of the first octet in the destination MAC.

The last type is for multicast and broadcast traffic, identified by a 1 in the last bit of the first octet of the destination MAC.

Our traffic is unicast icmp traffic, but lets first sidetrack to look at how ARP and non-unicast traffic is handled. To conserve on broadcast traffic, the router cloud, we have two options to enable the Neutron: L2 population and r2 responder.

Lets first look at table 21 for ARP traffic:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=21"`
Table 21 actually generates ARP responses for any ARP requests for VMs in our cloud. In our match fields, we wee we're looking for ARP traffic in this table. More specifically, we were looking for ARP requests for VMs, http services, and source NAT routers on remote physical hosts. If an ARP request comes into the tunneling bridge for these destinations, an ARP reply packet is forged. 

For handling broadcast traffic, we can look at table 22:
`[root@compute1 ~]#` `watch _n.5 "ovs-ofctl dump-flows br-tun table=22"`
In our examples, we only have 2 compute hosts, however, in a real cloud, you probably have many more.

Back to our ICMP flow. Starting with table 2 in the tunneling bridge, our ICMP traffic is unicast, so that matching flows in table 20:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=2"`
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=20"`

Port 5 is an ovs port used for a VXLan tunnel:
`[root@compute1 ~]#` `ovs-ofctl show br-tun`

Lets jump to compute2 now:
`[root@compute2 ~]#` `ovs-vsctl show`

With the following command, we can see this port is OpenFlow port 5:
`[root@compute1 ~]#` `ovs-ofctl show br-tun`

Lets see the OpenFlow entry:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=0"` 

For traffic coming in from the tunnel, we go to table 4:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=4"`
Here in table 4, we see a matching flow entry based on our VNI in hex format. The action is to set a VLAN tag of 2 and go to table 9:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=9"`
In table 9, the last line is a default to go to table 10. Now table 10 it's a single entry here. it's an entry that is used to learn the mapping of tunnels to VM ethernet addresses so when tunnel traffic arrives and hits the rule, the tunneling bridge will add an entry in table 20 on how to get back to that VM and which tunnel ID and tunnel port to use.

A quick look at table 20, and we can see the learned entry:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=20"` 
This is used on the return traffic to use the right tunnel ID and tunel port on return traffic.

Back to table 10:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=10"` 
After the tunnel to MAC mapping is learned, we finally get out of the tunneling bridge and go out OpenFlow port 1.

`[root@compute2 ~]#` `ovs-ofctl show br-tun`
Port 1 is the patch port that goes to the integration bridge. From the integration bridge, normal MAC forwarding occurs to get to the destination VM RED3. 

Now we'll deal with routing of traffic. 

Lets start pinging from RED1 to GREEN1 and dive in:
`$` `ping 10.2.2.4`

Lets capture this traffic:
`[root@compute1 ~]#` brctl show
With this command, we can see the associated Linux bridge for RED1. 

Lets run TCPdump on the tap port:
`[root@compute1 ~]#` `tcpdump -i tap0e6fb773-92 -n -e icmp`
We're capturing on the VM's tap port right before the Linux bridge. The -e option shows layer 2 MAC addressing information. 

Since traffic is going to another network, we need a router. The destination MAC address is the VM's default gateway, which is the Neutron router.

Lets jump into the actual router:
`[root@compute1 ~]#` `ip netns`
This shows us our network namespaces. There's 1 Neutron router here, the qrouter:
`[root@compute1 ~]#` `ip netns exec qrouter-983bb01f-5605-4fce-9bdd-df29a353562f bash`
This command gets us into the network namespace that is doing the routing. 

This command shows us the router links. 
`[root@compute1 ~]#` `ip a`

With ip route, we can see the routing for the router's interfaces:
`[root@compute1 ~]#` `ip route`

Lets see these two router interfaces in ovs:
`[root@compute1 ~]#` `ovs-vsctl show`

Now lets see what happens when routing occurs between VMs on different physical hosts. 

First lets ping from RED1 to GREEN2:
`$` `ping 10.2.2.5`

`[root@controll ~(keystone_demo)]#` `neutron l3-agent-list-hosting-router VR`
This command shows us which hosts contain our router. The result shows us our one writer is actually on all 3 of our nodes, revealing its distributed nature.

Lets observe the traffic flow. The traffic is first routed by the local distributed router on compute host 1. Lets observe traffic after it has been routed by the neutron router:
`[root@compute1 ~]#` `ip netns exec qrouter-983bb01f-5605-4fce-9bdd-df29a353562f \
> tcpdump -n -e -i qr-1a2e76b2-1a`

Lets drop into flow table 1 on the tunneling bridge:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=1"

After table 1, the flow is redirected to table 2. Table 2 separates unicast traffic for multicast and broadcast traffic:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=2"

Since this is unicast, we go to table 20:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=20"

Lets go to compute host 2. In compute host 2, we can look at the processing pipeline on the tunnel bridge:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=0"

Traffic coming in from a vxlan port is submitted to table 4:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=4"
Traffic is tagged with the vlan 1 and we go to table 9.

At table 9, we match on traffic from the source DVR MAC of compute host 1 and send out OpenFlow port 1. Port 1 is a patch port to the integration bridge:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=9"

On the integration bridge, we match on the DVR MAC of compute host 1 and punt to table 1:
`[root@compute2 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-tun table=1"

----

Lets follow traffic from red1 to 200.0.0.1, the provider network default gateway. First we'll start a ping from red1 to the gateway and see that works:
`$` `ping 200.0.0.1`

Here we are on compute host 1. This is the shell of the Neutron router:
`[root@compute1 ~]#` `ip netns exec qrouter-983bb01f-5605-4fce-9bdd-df29a353562f bash`

IP route shows there is actually no route in the main routing table to reach 200.0.0.1. 
`[root@compute1 ~]#` `ip route`

We can look for other routing policies with the following command:
`[root@compute1 ~]#` `ip rule show`

Lets check table 16 with the following command:
`[root@compute1 ~]#` `ip route show table 16`
We can see a default route.

In addition to the route, we have NAT rules on the Neutron router that we can observe with the following command:
`[root@compute1 ~]#` `iptables -S -t nat`

With the following command, we go into the shell of the floating IP namespace.
`[root@compute1 ~]#` `ip netns`

The floating IP namespace has a default route:
`[root@compute1 ~]#` `ip route`

This command shows the fg interfaces using an IP address from our external network of 200.0.0.4:
`[root@compute1 ~]#` `ip a`

This command reveals this fg interface is on the integration bridge:
`[root@compute1 ~]#` `ovs-vsctl show`

In table 0, we have a rule for traffic in VLAN 3:
`[root@compute1 ~]#` `watch -n.5 "ovs-ofctl dump-flows br-ex table=0"

Now lets look at traffic initiated from outside our cloud towards the floating IP:
`[root@compute1 ~]#` `ip netns exec fip-6abc926e-f5a5-4cf7-a723-6508ee900de8 bash`

We can see this from the shell of the floating IP namespace:
`[root@compute1 ~]#` `ip link`

Here we see proxy ARP set to 1 or enabled:
`[root@compute1 ~]#` `cat /proc/sys/net/ipv4/conf/fg-18330731-dc/proxy_arp`

With IP route, the floating IP namespace, we see a static route to get to red1's floating IP link to the Neutron router.
`[root@compute1 ~]#` `ip route`

On the shell of the Neutron router, we can look at the relevant iptables rule:
`[root@compute1 ~]#` `iptables -S -t nat`

Lets SSH from red1 to red2's fixed IP:
`$` ssh `10.0.0.5`

Now in RED2, we initiate a ping towards 200.0.0.1, the provider network gateway:
`$` `ip a`
`$` `ping 200.0.0.1`

Lets go to the distributed Neutron router on compute host 1 and check routing:
`[root@compute1 ~]#` `ip netns exec qrouter-983bb01f-5605-4fce-9bdd-df29a353562f bash`

With ip rule list, we can see the policy routing rules:
`[root@compute1 ~]#` `ip rule list`

Next, there is a policy route that matches on all traffic from 10.0.0.0/24:
`[root@compute1 ~]#` `ip route show table 167772161`

Lets look at the snat namespace on the controller where the network node is IP 
`[root@controller ~(keystone_demo)]#` `ip netns`
This command shows the source NAT namespace.

Lets go into the shell for this namespace:
`[root@controller ~(keystone_demo)]#` `ip netns exec snat-983bb01f-5605-4fce-9bdd-df29a353562f bash`

`[root@controller ~(keystone_demo)]#` `ip a`
ip a shows 2 sg interfaces; one is in the RED network and one is in the green network.

For traffic following the default route, there's a source NAT rule in this source NAT namespace:
`[root@controller ~(keystone_demo)]#` `iptables --list -t nat`
With iptables, we can see this source NAT rule.

For traffic bound to external destinations, it is source NATTED:
`[root@controller ~(keystone_demo)]#` `ip a`

After source nat, traffic is sent out the qg interface:
`[root@controller ~(keystone_demo)]#` `ip route`

The qg interface is attached to the integration bridge in vlan3:
`[root@controller ~(keystone_demo)]#` `ovs-vsctl show | more 

Lets check the OpenFlow rules on the external bridge:
`[root@controller ~(keystone_demo)]#` `watch -n.5 "ovs-ofctl dump-flows br-ex table=0"`

One of the external bridge's ports is a NIC of the network node:
`[root@controller ~(keystone_demo)]#` `ovs-vsctl list-ports br-ex`



